{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "import nltk\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = 'Dataset/images'\n",
    "\n",
    "path_to_train = 'Dataset/train_df.tsv'\n",
    "\n",
    "path_to_val = 'Dataset/val_df.tsv'\n",
    "\n",
    "path_to_test = 'Dataset/test_df.tsv'\n",
    "\n",
    "path_to_save_model = 'saved models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEDataset(Dataset):\n",
    "    def __init__(self, path_to_data_df, path_to_images, tokenizer, image_transform):\n",
    "        self.data = pd.read_csv(path_to_data_df, sep='\\t', names=['pid', 'text', 'explanation'])\n",
    "        self.path_to_images = path_to_images\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx, :]\n",
    "\n",
    "        pid_i = row['pid']\n",
    "        src_text = row['text']\n",
    "        target_text = row['explanation']\n",
    "\n",
    "        max_length = 256\n",
    "        encoded_dict = tokenizer(\n",
    "            src_text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            add_prefix_space = True\n",
    "        )\n",
    "        src_ids = encoded_dict['input_ids'][0]\n",
    "        src_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        image_path = os.path.join(self.path_to_images, pid_i+'.jpg')\n",
    "        img = np.array(Image.open(image_path).convert('RGB'))\n",
    "        img_inp = self.image_transform(img)\n",
    "        \n",
    "\n",
    "        encoded_dict = tokenizer(\n",
    "          target_text,\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\",\n",
    "          truncation=True,\n",
    "          return_tensors='pt',\n",
    "          add_prefix_space = True\n",
    "        )\n",
    "\n",
    "        target_ids = encoded_dict['input_ids'][0]\n",
    "\n",
    "        sample = {\n",
    "            \"input_ids\": src_ids,\n",
    "            \"attention_mask\": src_mask,\n",
    "            \"input_image\": img_inp,\n",
    "            \"target_ids\": target_ids,\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, path_to_train_df, path_to_val_df, path_to_test_df, path_to_images, tokenizer, image_transform, batch_size=16):\n",
    "        super(MSEDataModule, self).__init__()\n",
    "        self.path_to_train_df = path_to_train_df\n",
    "        self.path_to_val_df = path_to_val_df\n",
    "        self.path_to_test_df = path_to_test_df\n",
    "        self.path_to_images = path_to_images\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "  \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = MSEDataset(self.path_to_train_df, self.path_to_images, self.tokenizer, self.image_transform)\n",
    "        self.val_dataset = MSEDataset(self.path_to_val_df, self.path_to_images, self.tokenizer, self.image_transform)\n",
    "        self.test_dataset = MSEDataset(self.path_to_test_df, self.path_to_images, self.tokenizer, self.image_transform)\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, sampler = RandomSampler(self.train_dataset), batch_size = self.batch_size)\n",
    "  \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size = self.batch_size)\n",
    "  \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Multimodal Sarcasm Detection Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartModel, AdamW, BartConfig, BartPretrainedModel\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers.generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "from transformers.generation_logits_process import (\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SequenceClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "def getClones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "class CrossmodalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, img_model=512, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(img_model, d_model)\n",
    "        self.k_linear = nn.Linear(img_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        #d_ff is set as default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "class CrossmodalEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, img_model=512, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = CrossmodalMultiHeadAttention(heads, d_model, img_model=img_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text_feats, img_feats, mask):\n",
    "        x = text_feats\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,img_feats,img_feats))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class CrossmodalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, img_model=512, heads=4, N=1, dropout=0.1):\n",
    "        super(CrossmodalEncoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.cme_layers = getClones(CrossmodalEncoderLayer(d_model, heads, img_model=img_model, dropout=dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    \n",
    "    def forward(self, text_feats, img_feats, mask):\n",
    "        x = text_feats\n",
    "        for i in range(self.N):\n",
    "            x = self.cme_layers[i](x, img_feats, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class MultimodalBartEncoder(nn.Module):\n",
    "    def __init__(self, bart_encoder, bart_config, image_encoder, img_model=512, N=1, heads=4, dropout=0.1):\n",
    "        super(MultimodalBartEncoder, self).__init__()\n",
    "        self.config = bart_config\n",
    "        self.bart_encoder = bart_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.N=N\n",
    "        self.img_model = img_model\n",
    "        self.cross_modal_encoder = CrossmodalEncoder(self.config.d_model, img_model=img_model, heads=heads, N=N, dropout=dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        image_features=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        ):\n",
    "            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "            \n",
    "            vgg_image_features = self.image_encoder(image_features)\n",
    "            \n",
    "            vgg_image_features = vgg_image_features.permute(0, 2, 3, 1)\n",
    "            vgg_image_features = vgg_image_features.view(\n",
    "                -1, \n",
    "                vgg_image_features.size()[1]*vgg_image_features.size()[2], \n",
    "                self.img_model\n",
    "                )\n",
    "            \n",
    "            encoder_outputs = self.bart_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            \n",
    "            cross_modal_encoder_outputs = self.cross_modal_encoder(\n",
    "                encoder_outputs.last_hidden_state, \n",
    "                vgg_image_features,\n",
    "                attention_mask\n",
    "            )\n",
    "            \n",
    "            encoder_outputs.last_hidden_state = torch.cat((encoder_outputs.last_hidden_state, cross_modal_encoder_outputs), dim=-2)\n",
    "            return encoder_outputs\n",
    "\n",
    "class BartClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        inner_dim: int,\n",
    "        num_classes: int,\n",
    "        pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.out_proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BartForMultimodalSarcasmDetection(BartPretrainedModel):\n",
    "    def __init__(self, bart_model_encoder, bart_config, image_encoder, num_labels=2, dropout_rate=0.1, img_model=512, N=1, heads=4):\n",
    "        super(BartForMultimodalSarcasmDetection, self).__init__(bart_config)\n",
    "        self.config = bart_config\n",
    "        self.encoder = MultimodalBartEncoder(bart_model_encoder, bart_config, image_encoder, img_model=img_model, N=N, heads=heads, dropout=dropout_rate)\n",
    "        self.classification_head = BartClassificationHead(\n",
    "            self.config.d_model,\n",
    "            self.config.d_model,\n",
    "            num_labels,\n",
    "            dropout_rate,\n",
    "        )\n",
    "        self._init_weights(self.classification_head.dense)\n",
    "        self._init_weights(self.classification_head.out_proj)\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        image_features = None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            image_features=image_features,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        classification_feature_vector = encoder_outputs.last_hidden_state.mean(dim=-2)\n",
    "        logits = self.classification_head(classification_feature_vector)\n",
    "        loss = None\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=encoder_outputs.last_hidden_state,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning model for Multimodal Sarcasm Detection Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyLitModel(pl.LightningModule):\n",
    "    def __init__(self, model, hparams):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.hparams = hparams\n",
    "\n",
    "        if self.hparams['freeze_encoder']:\n",
    "            freeze_params(self.model.encoder.bart_encoder)\n",
    "\n",
    "        if self.hparams['freeze_embeds']:\n",
    "            self.freeze_embeds()\n",
    "    \n",
    "    def freeze_embeds(self):\n",
    "        ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n",
    "        freeze_params(self.model.bart_model_shared)\n",
    "        for d in [self.model.encoder.bart_encoder, self.model.decoder]:\n",
    "            freeze_params(d.embed_positions)\n",
    "            freeze_params(d.embed_tokens)\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": self.model.encoder.cross_modal_encoder.parameters(), \"lr\": self.hparams['lr']},\n",
    "                {\"params\": self.model.classification_head.parameters(), \"lr\": self.hparams['lr']},\n",
    "            ],\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src_ids, src_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        image_features = batch['input_image'].to(device)\n",
    "        labels = batch['target_ids'].to(device)\n",
    "        \n",
    "        outputs = self(src_ids, attention_mask=src_mask, image_features=input_images, use_cache=False)\n",
    "        classification_logits = outputs.logits\n",
    "        \n",
    "        # The loss function\n",
    "        ce_loss = torch.nn.CrossEntropyLoss() #ignore_index=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        # Calculate the loss on the un-shifted tokens\n",
    "        loss = ce_loss(classification_logits.view(-1, classification_logits.shape[-1]), labels.view(-1))\n",
    "        \n",
    "        self.log('train_cross_entropy_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss':loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        src_ids = batch['input_ids'].to(device)\n",
    "        src_mask = batch['attention_mask'].to(device)\n",
    "        image_features = batch['input_image'].to(device)\n",
    "        labels = batch['target_ids'].to(device)\n",
    "                \n",
    "        outputs = self(src_ids, attention_mask=src_mask, image_features=input_images, use_cache=False)\n",
    "        classification_logits = outputs.logits\n",
    "\n",
    "        ce_loss = torch.nn.CrossEntropyLoss() #ignore_index=self.tokenizer.pad_token_id)\n",
    "        val_loss = ce_loss(classification_logits.view(-1, classification_logits.shape[-1]), labels.view(-1))\n",
    "        \n",
    "        self.log('val_cross_entropy_loss', val_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_f1_score', f1(F.softmax(classification_logits, dim=1), labels, num_classes=2), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': val_loss}\n",
    "    \n",
    "    def predict(self, src_ids, src_mask, input_images):\n",
    "        src_ids = src_ids.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "        input_images = input_images.to(device)\n",
    "\n",
    "        outputs = self(src_ids, attention_mask=src_mask, input_images=input_images, use_cache=False)\n",
    "        classification_logits = outputs.logits\n",
    "        class_probs = F.softmax(classification_logits, dim=1)\n",
    "        return torch.argmax(class_probs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Model - ExMore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n",
    "    \"\"\"\n",
    "    prev_output_tokens = input_ids.clone()\n",
    "\n",
    "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n",
    "\n",
    "    index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "    decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n",
    "    prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n",
    "    prev_output_tokens[:, 0] = decoder_start_tokens\n",
    "\n",
    "    return prev_output_tokens\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqLMOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    \n",
    "\n",
    "class BartForMultimodalSarcasmExplanation(BartPretrainedModel):\n",
    "    def __init__(self, multimodal_bart_encoder_TL, bart_decoder, bart_config, bart_model_num_embs, img_model=512, N=1, heads=4):\n",
    "        super(BartForMultimodalSarcasmExplanation, self).__init__(bart_config)\n",
    "        self.config = bart_config\n",
    "        self.encoder = multimodal_bart_encoder_TL\n",
    "        self.decoder = bart_decoder\n",
    "        self.lm_head = nn.Linear(self.config.d_model, bart_model_num_embs) #, bias=False)\n",
    "        \n",
    "        self._init_weights(self.lm_head)\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "    \n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids, past=None, \n",
    "        attention_mask=None, \n",
    "        use_cache=None, \n",
    "        encoder_outputs=None, \n",
    "        image_features=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"image_features\": image_features,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "        }\n",
    "    \n",
    "    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n",
    "        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\n",
    "            self._force_token_id_to_be_generated(logits, self.config.bos_token_id)\n",
    "        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
    "            self._force_token_id_to_be_generated(logits, self.config.eos_token_id)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def _force_token_id_to_be_generated(scores, token_id) -> None:\n",
    "        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float(\"inf\"))\"\"\"\n",
    "        scores[:, [x for x in range(scores.shape[1]) if x != token_id]] = -float(\"inf\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "\n",
    "        image_features = None,\n",
    "        \n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None\n",
    "    ):\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            decoder_input_ids = shift_tokens_right(input_ids, self.config.pad_token_id)\n",
    "        \n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                image_features=image_features,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        \n",
    "        enc_attn_mask = torch.cat((attention_mask, attention_mask), dim=-1)\n",
    "        \n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "            encoder_attention_mask=enc_attn_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state, # also carries crossmodal_encoder_last_hidden_state concatenated.\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pytorch Lightning - Main Model - ExMore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyLitBartForMultimodalSarcasmExplanation(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, hparams):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        if self.hparams['freeze_image_encoder']:\n",
    "            freeze_params(self.model.encoder.image_encoder)\n",
    "        \n",
    "        if self.hparams['freeze_encoder']:\n",
    "            freeze_params(self.model.encoder.bart_encoder)\n",
    "\n",
    "        if self.hparams['freeze_embeds']:\n",
    "            self.freeze_embeds()\n",
    "  \n",
    "    def freeze_embeds(self):\n",
    "        ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n",
    "        freeze_params(self.model.bart_model_shared)\n",
    "        for d in [self.model.encoder.bart_encoder, self.model.decoder]:\n",
    "            freeze_params(d.embed_positions)\n",
    "            freeze_params(d.embed_tokens)\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "  \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "          [\n",
    "              {\"params\": self.model.encoder.cross_modal_encoder.parameters(), \"lr\": self.hparams['lr_finetune_cm']},\n",
    "              {\"params\": self.model.lm_head.parameters(), \"lr\": self.hparams['lr']},\n",
    "          ],\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        src_ids, src_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        image_features = batch['input_image'].to(device)\n",
    "        tgt_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "        decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "\n",
    "        # Run the model and get the logits\n",
    "        outputs = self(src_ids, attention_mask=src_mask, image_features=image_features, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "        lm_logits = outputs.logits\n",
    "        \n",
    "        # the loss function\n",
    "        ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        # Calculate the loss on the un-shifted tokens\n",
    "        loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss':loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src_ids = batch['input_ids'].to(device)\n",
    "        src_mask = batch['attention_mask'].to(device)\n",
    "        image_features = batch['input_image'].to(device)\n",
    "        tgt_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "\n",
    "        # Run the model and get the logits\n",
    "        outputs = self(src_ids, attention_mask=src_mask, image_features=image_features, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "        lm_logits = outputs.logits\n",
    "\n",
    "        ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "        self.log('val_loss', val_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': val_loss}\n",
    "  \n",
    "    # This method generates text using the BartForConditionalGeneration's generate() method\n",
    "    def generate_text(self, text, eval_beams, image_features=None, early_stopping = True, max_len = 40):\n",
    "        ''' Function to generate text '''\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"image_features\": image_features\n",
    "        }\n",
    "        generated_ids = self.model.generate(\n",
    "            text[\"input_ids\"],\n",
    "            attention_mask=text[\"attention_mask\"],\n",
    "            use_cache=True,\n",
    "            decoder_start_token_id = self.tokenizer.pad_token_id,\n",
    "            num_beams= eval_beams,\n",
    "            max_length = max_len,\n",
    "            early_stopping = early_stopping,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        return [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
    "\n",
    "def freeze_params(model):\n",
    "    ''' This function takes a model or its subset as input and freezes the layers for faster training\n",
    "      adapted from finetune.py '''\n",
    "    for layer in model.parameters():\n",
    "        layer.requires_grade = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_encoder():\n",
    "    vgg19model = models.vgg19(pretrained=True)\n",
    "    image_encoder = list(vgg19model.children())[0]\n",
    "    return image_encoder\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                               \n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=True)\n",
    "\n",
    "bart_model = BartModel.from_pretrained('facebook/bart-base')\n",
    "\n",
    "bart_config = BartConfig.from_pretrained(\"facebook/bart-base\", return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = load_image_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'freeze_encoder': False,\n",
    "    'freeze_embeds': False,\n",
    "    'freeze_image_encoder': True,\n",
    "    'eval_beams': 4,\n",
    "    'lr_finetune_cm':1e-5, #for crossmodal encoder\n",
    "    'lr': 3e-4, #for lm_head\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model_for_msd = BartForMultimodalSarcasmDetection(\n",
    "    bart_model.get_encoder(), \n",
    "    bart_config, \n",
    "    image_encoder, \n",
    "    num_labels=2,\n",
    "    dropout_rate=0.1,\n",
    "    img_model=512,\n",
    "    N=1,\n",
    "    heads=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_checkpoint_path = 'please enter the msd pretrained checkpoint path here'\n",
    "pylit_bart_model_for_msd = PyLitModel.load_from_checkpoint(checkpoint_path=msd_checkpoint_path, \n",
    "                                      model = bart_model_for_msd, \n",
    "                                      hparams = hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_bart_encoder_TL = pylit_bart_model_for_msd.model.get_encoder()\n",
    "bart_decoder = bart_model.get_decoder()\n",
    "bart_model_num_embs = bart_model.shared.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_for_mse = BartForMultimodalSarcasmExplanation(multimodal_bart_encoder_TL, \n",
    "                                            bart_decoder, bart_config, \n",
    "                                            bart_model_num_embs, img_model=512, N=1, heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into the model for training\n",
    "\n",
    "mse_data = MSEDataModule(path_to_train, path_to_val, \n",
    "                         path_to_test, path_to_images, \n",
    "                         tokenizer, image_transform, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from a pre-saved checkpoint or use the code below to start training from scratch\n",
    "\n",
    "main_model = PyLitBartForMultimodalSarcasmExplanation(tokenizer = tokenizer, model = bart_for_mse, hparams = hparams)\n",
    "\n",
    "# model = PyLitBartForMultimodalSarcasmExplanation.load_from_checkpoint(checkpoint_path=\"ckpt path\",\n",
    "                                    #   tokenizer = tokenizer, model = bart_for_mse, hparams = hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(path_to_save_model, 'model_dir')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=ckpt_dir,\n",
    "    monitor='val_loss',\n",
    "    save_top_k=15,\n",
    "    mode='min',\n",
    "    filename='{epoch}-{val_loss:.3f}'\n",
    "    )\n",
    "tb_logger = pl_loggers.TensorBoardLogger(os.path.join(ckpt_dir, 'logs/'))\n",
    "trainer = pl.Trainer(\n",
    "                    logger=tb_logger,\n",
    "                    gpus = 1,\n",
    "                    max_epochs = 125,\n",
    "                    min_epochs = 5,\n",
    "                    auto_lr_find = False,\n",
    "                    checkpoint_callback = checkpoint,\n",
    "                    progress_bar_refresh_rate = 10\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the instantiated model to the data\n",
    "trainer.fit(main_model, mse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to manually save a checkpoint, although the model should automatically save checkpoints as it moves through the epochs\n",
    "trainer.save_checkpoint(os.path.join(ckpt_dir,\"last_epoch_125.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'Enter the path of ExMore model checkpoint here.'\n",
    "main_model = PyLitBartForMultimodalSarcasmExplanation.load_from_checkpoint(checkpoint_path=ckpt_path,\n",
    "                                      tokenizer = tokenizer, model = bart_for_mse, hparams = hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(path_to_test, sep='\\t', header=None)\n",
    "# test = pd.read_csv(path_to_train, sep='\\t', header=None)\n",
    "test.columns = ['pid', 'source', 'target']\n",
    "pids = test.pid.tolist()\n",
    "source = test.source.tolist()\n",
    "target = test.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model.to(device)\n",
    "main_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_beams=4\n",
    "pred = []\n",
    "\n",
    "for pid_i, src, tgt in tqdm(zip(pids, source, target)):\n",
    "    encoded_dict = tokenizer(\n",
    "      src,\n",
    "      max_length=256,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_tensors='pt',\n",
    "      add_prefix_space = True\n",
    "    )\n",
    "    encoded_dict['input_ids'] = encoded_dict['input_ids'].to(device)\n",
    "    encoded_dict['attention_mask'] = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    if type(pid_i) is not str:\n",
    "        pid_i = str(pid_i)\n",
    "    \n",
    "    image_path = os.path.join(path_to_images, pid_i+'.jpg')\n",
    "    img = np.array(Image.open(image_path).convert('RGB'))\n",
    "    img_feats = image_transform(img)\n",
    "    img_feats = img_feats.unsqueeze(0)\n",
    "    \n",
    "    gen = main_model.generate_text(\n",
    "      encoded_dict, \n",
    "      eval_beams, \n",
    "      image_features=img_feats.to(device), \n",
    "      early_stopping = True, \n",
    "      max_len = 256\n",
    "    )\n",
    "    \n",
    "    pred.append(gen[0])\n",
    "    hypothesis = gen[0].split()\n",
    "    reference = tgt.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = pd.DataFrame({0:pids, 1:source, 2:target, 3:pred})\n",
    "predictions_1\n",
    "\n",
    "predictions = pd.DataFrame({0:target, 1:pred})\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_predictions = 'Enter the path to save the predictions file'\n",
    "predictions_1.to_csv(path_to_predictions, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy import spatial\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_preds = \"Enter path to the predictions.tsv file to compute evalusation scores\"\n",
    "predictions = pd.read_csv(path_to_preds, sep=\"\\t\", header=None)\n",
    "predictions.columns = ['pid', 'source', 'reference', 'hypothesis']\n",
    "y_true = predictions.reference.tolist()\n",
    "y_pred = predictions.hypothesis.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test_ocr_df = 'Enter path to the test_set_ocr.tsv file to compute evalusation scores'\n",
    "test_ocr_df = pd.read_csv(path_to_test_ocr_df, sep=\"\\t\", header=None)\n",
    "test_ocr_df.columns = ['pid', 'source', 'reference']\n",
    "\n",
    "predictions_ocr = predictions[predictions['pid'].isin(test_ocr_df['pid'])]\n",
    "y_true = predictions_ocr.reference.tolist()\n",
    "y_pred = predictions_ocr.hypothesis.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_ocr.to_csv(\"path to save predictions_test_ocr.tsv\",\n",
    "#                      sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test_non_ocr_df = 'Enter path to the test_set_non_ocr.tsv file to compute evaluation scores'\n",
    "test_non_ocr_df = pd.read_csv(path_to_test_non_ocr_df, sep=\"\\t\", header=None)\n",
    "test_non_ocr_df.columns = ['pid', 'source', 'reference']\n",
    "\n",
    "predictions_non_ocr = predictions[predictions['pid'].isin(test_non_ocr_df['pid'])]\n",
    "y_true = predictions_non_ocr.reference.tolist()\n",
    "y_pred = predictions_non_ocr.hypothesis.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_non_ocr.to_csv(\"path to save predictions_test_non_ocr.tsv\",\n",
    "#                          sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_1 = 0\n",
    "bleu_2 = 0\n",
    "bleu_3 = 0\n",
    "bleu_4 = 0\n",
    "count = 0\n",
    "weights_1 = (1./1.,)\n",
    "weights_2 = (1./2. , 1./2.)\n",
    "weights_3 = (1./3., 1./3., 1./3.)\n",
    "weights_4 = (1./4., 1./4., 1./4., 1./4.)\n",
    "met = 0\n",
    "\n",
    "for reference, hypothesis in zip(y_true, y_pred):\n",
    "    met += nltk.translate.meteor_score.meteor_score([reference], hypothesis)\n",
    "    reference = reference.split()\n",
    "    hypothesis = hypothesis.split()\n",
    "    bleu_1 += sentence_bleu([reference], hypothesis, weights_1) \n",
    "    bleu_2 += sentence_bleu([reference], hypothesis, weights_2)\n",
    "    bleu_3 += sentence_bleu([reference], hypothesis, weights_3)\n",
    "    bleu_4 += sentence_bleu([reference], hypothesis, weights_4)\n",
    "    count += 1\n",
    "\n",
    "bleu_1 = bleu_1/count\n",
    "bleu_2 = bleu_2/count\n",
    "bleu_3 = bleu_3/count\n",
    "bleu_4 = bleu_4/count\n",
    "met = met/count\n",
    "print(\"BLEU-1:\", bleu_1)\n",
    "print(\"BLEU-2:\", bleu_2)\n",
    "print(\"BLEU-3:\", bleu_3)\n",
    "print(\"BLEU-4:\", bleu_4)\n",
    "print(\"METEOR:\", met)\n",
    "\n",
    "rouge1 = 0\n",
    "rouge2 = 0\n",
    "rougel = 0\n",
    "count = 0\n",
    "\n",
    "for reference, hypothesis in zip(y_true, y_pred):\n",
    "\tcount += 1\n",
    "\tscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\tscores = scorer.score(reference, hypothesis)\n",
    "\trouge1 += scores['rouge1'].fmeasure\n",
    "\trouge2 += scores['rouge2'].fmeasure\n",
    "\trougel += scores['rougeL'].fmeasure\n",
    "\n",
    "rouge1 = rouge1/count\n",
    "rouge2 = rouge2/count\n",
    "rougel = rougel/count\n",
    "print(\"ROUGE-1:\", rouge1)\n",
    "print(\"ROUGE-2:\", rouge2)\n",
    "print(\"ROUGE-L:\", rougel)\n",
    "\n",
    "P, R, F1 = score(y_pred, y_true, lang=\"en\", verbose=True)\n",
    "\n",
    "print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "print(f\"System level P score: {P.mean():.3f}\")\n",
    "print(f\"System level R score: {R.mean():.3f}\")\n",
    "\n",
    "# Cosine Similarity for Sentence BERT representation\n",
    "sentence_embeddings = sentence_transformer_model.encode(y_true)\n",
    "sentence_embeddings2 = sentence_transformer_model.encode(y_pred)\n",
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = torch.mean(cos(torch.tensor(sentence_embeddings), torch.tensor(sentence_embeddings2))).item()\n",
    "print(\"Cosine Similarity of Sentence Representation using S-BERT:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
